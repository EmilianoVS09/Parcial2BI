'''
Justificaci√≥n de las transformaciones (en funci√≥n de las preguntas de negocio)

Este pipeline prepara y enriquece tres colecciones de Airbnb (listings, calendar y reviews) para poder responder, con datos consistentes y comparables, a preguntas como: la relaci√≥n entre verificaciones y calificaciones del anfitri√≥n, la localizaci√≥n y temporada alta/ baja, el v√≠nculo precio‚Äìocupaci√≥n en el tiempo, el impacto de las tasas de respuesta/aceptaci√≥n, el efecto de ser superhost, la actividad reciente de reviews, la limpieza/comunicaci√≥n, la ocupaci√≥n anual y la variaci√≥n de precio, entre otras. A continuaci√≥n se explica, paso a paso, por qu√© se aplica cada transformaci√≥n.

1) Normalizaci√≥n de tipos (fechas, precios, porcentajes, texto) y descarte de columnas de ruido

Fechas ‚Üí ISO (YYYY-MM-DD) en listings, calendar y reviews: necesitamos una base temporal homog√©nea para detectar temporada alta/baja y comparar m√©tricas por mes/quarter/fin de semana (p. ej., ¬øqu√© colonias concentran mejores calificaciones en temporada alta? y ¬øc√≥mo var√≠a la ocupaci√≥n a lo largo del tiempo?). Tambi√©n habilita ventanas m√≥viles como ‚Äú√∫ltimos 30 d√≠as‚Äù en rese√±as.

Precios ‚Üí num√©rico (price_num): imprescindible para estimar ingresos y estudiar la relaci√≥n precio‚Äìocupaci√≥n en series diarias y por segmentos (¬øc√≥mo var√≠a el precio en temporada baja?, ¬øexiste correlaci√≥n entre calificaciones altas y precios m√°s elevados?).

Porcentajes ‚Üí num√©rico (host_response_rate_pct, host_acceptance_rate_pct): permite medir la influencia de la tasa de aceptaci√≥n/ respuesta del anfitri√≥n en la ocupaci√≥n sin sesgos por formatos de texto.

Limpieza de texto (remover HTML, normalizar Unicode): necesario para segmentar correctamente por colonia/barrio y tipo de habitaci√≥n, y para reportes interpretables cuando comparamos ubicaci√≥n, amenities o atributos del host con calificaci√≥n y ocupaci√≥n.

Eliminar columnas de calendar no usadas (adjusted_price, _id): reduce ruido que podr√≠a distorsionar m√©tricas de precio diario y ocupaci√≥n.

2) Limpieza de nulos e imputaciones jer√°rquicas (con banderas de faltantes)

Protecci√≥n de claves (id, host_id) y tipificaci√≥n num√©rica previa: asegura joins correctos y c√°lculos confiables, evitando p√©rdidas de muestra que sesgar√≠an resultados al comparar colonias o periodos (impacto directo sobre precio‚Äìocupaci√≥n, ocupaci√≥n anual y temporada alta/baja).

Estandarizaci√≥n de NA en textos y rellenos b√°sicos (p. ej., host_name, host_response_time): evita categor√≠as fantasma y mantiene la posibilidad de analizar el rol del host (p. ej., ser superhost o su tiempo de respuesta) frente a ocupaci√≥n y calificaciones.

Banderas _was_missing: dejan trazabilidad de d√≥nde se imput√≥, clave para auditar si una relaci√≥n (p. ej., verificaci√≥n ‚Üî calificaci√≥n o precio ‚Üî ocupaci√≥n) podr√≠a estar influida por datos faltantes.

Imputaci√≥n de tasas del host (respuesta/aceptaci√≥n): en cascada host ‚Üí grupo (barrio/tipo/aforo) ‚Üí global y recorte a [0,100]. Esta estrategia mantiene comparabilidad entre listings para analizar c√≥mo influyen estas tasas en la ocupaci√≥n sin inflar/deflactar extremos.

Imputaci√≥n de review_scores_*: por grupo y barrio, luego global, y winsorization p1‚Äìp99. Esto estabiliza las calificaciones (limpieza, comunicaci√≥n, etc.) frente a outliers, permitiendo evaluar de forma robusta:

¬øLa verificaci√≥n del anfitri√≥n se asocia a mejores calificaciones?

¬øSer superhost impacta calificaci√≥n y ocupaci√≥n?

¬øExiste correlaci√≥n entre calificaciones altas y precios m√°s elevados?

reviews_per_month ‚Üí 0.0: evita perder propiedades en an√°lisis de actividad reciente (top rese√±as √∫ltimos 30 d√≠as) y su relaci√≥n con ocupaci√≥n e ingresos.

Coordenadas (latitude/longitude) imputadas por barrio y global: imprescindibles para analizar localizaci√≥n ‚Üî precio promedio, comparar colonias y detectar tendencias por zona (ocupaci√≥n alta/ baja).

Contadores y disponibilidades (availability, n√∫mero de rese√±as, accommodates, etc.) imputados por grupo y global, y convertidos a enteros ‚â• 0: aseguran que medidas como ocupaci√≥n anual y disponibilidad en temporada baja sean comparables entre propiedades.

Booleanos y categ√≥ricos (superhost, verificaci√≥n de identidad, instant bookable, room_type, neighbourhood_cleansed): rellenados de forma consistente para habilitar cortes como superhost vs no superhost en ocupaci√≥n y calificaciones, o tipos de habitaci√≥n y colonias frente a precio y ocupaci√≥n.

host_since con fallback (last_scraped ‚Üí fecha fija): permite usar antig√ºedad del host como factor explicativo en ocupaci√≥n y calificaci√≥n, evitando huecos que invaliden comparaciones.

amenities y host_verifications forzados a listas: precondici√≥n para crear dummies controlados. Sin esto no podr√≠amos estimar el aporte de amenities clave a ocupaci√≥n ni la relaci√≥n verificaci√≥n ‚Üî calificaci√≥n del anfitri√≥n.

Calendar: available normalizado (t/f ‚Üí bool) y noches m√≠n/max: available es el n√∫cleo del c√°lculo de ocupaci√≥n (y por extensi√≥n ingresos). La normalizaci√≥n asegura m√©tricas temporales confiables; las noches m√≠n/max evitan valores err√°ticos en temporadas concretas.

Reviews: drops por campos cr√≠ticos y rellenos base: garantiza que conteos por mes (y ‚Äú√∫ltimos 30 d√≠as‚Äù) sean coherentes, fundamentales para evaluar si m√°s rese√±as recientes se asocian a mayor ocupaci√≥n e ingresos.

Red de seguridad (fill-all): completa nulos residuales por tipo para que ning√∫n an√°lisis tem√°tico (colonias, precio‚Äìocupaci√≥n, superhost, verificaci√≥n, limpieza/comunicaci√≥n) quede invalidad–æ por NaNs.

3) Derivaci√≥n de rasgos temporales y buckets de precio

Partes de fecha (a√±o, mes, quarter, d√≠a de semana, fin de semana): habilitan cortes inteligentes para temporada alta/baja, patrones por fin de semana, y an√°lisis longitudinal precio‚Äìocupaci√≥n (¬øqu√© colonias concentran mejores calificaciones en temporada alta? y ¬øc√≥mo var√≠a el precio/ocupaci√≥n durante el a√±o?).

Buckets de precio (quintiles o bins fijos) en listings y calendar: estratifican el mercado para comparar ocupaci√≥n e ingresos por nivel de precio, y estudiar si calificaciones altas se asocian a precios m√°s altos sin que unos pocos valores extremos dominen el an√°lisis.

4) Expansi√≥n controlada de campos anidados (amenities y verificaciones)

Amenidades seleccionadas

No se incluyen todas las amenidades del dataset, sino un conjunto curado de aquellas que inciden directamente en la experiencia del hu√©sped y, por tanto, en calificaciones, ocupaci√≥n e ingresos.

Conectividad y trabajo: Wifi, Dedicated workspace
‚Üí La presencia de wifi y espacios de trabajo influye en la ocupaci√≥n de largo plazo y en la preferencia de viajeros de negocios.

Comodidad y autoservicio: Self check-in, Free parking, Kitchen, Refrigerator, Microwave, Stove, Coffee maker, Cooking basics, Dishes and silverware
‚Üí Estas amenidades facilitan la autonom√≠a del hu√©sped y suelen correlacionarse con calificaciones m√°s altas y mayor ocupaci√≥n, especialmente en estancias familiares o prolongadas.

Confort y descanso: Room-darkening shades, Essentials, Washer
‚Üí Representan condiciones b√°sicas que impactan la satisfacci√≥n y, por tanto, las rese√±as de limpieza y comodidad.

Seguridad y confianza: Smoke alarm, Carbon monoxide alarm, Fire extinguisher, First aid kit
‚Üí Son determinantes en la percepci√≥n de seguridad, que puede reflejarse en mejores puntuaciones de valor y limpieza.

La selecci√≥n equilibra tres ejes: usabilidad (comodidades funcionales), seguridad y valor percibido, alineados con las preguntas sobre ocupaci√≥n, calificaci√≥n y precios.
Reducir el set a estos √≠tems evita dimensionalidad excesiva y mantiene solo aquellos con valor interpretativo en los an√°lisis.

Dummies de host_verifications (tel√©fono, email, work_email): instrumentan de manera directa la pregunta ¬øqu√© relaci√≥n existe entre la verificaci√≥n del anfitri√≥n y la calificaci√≥n promedio? y permiten controlar por este factor cuando se estudia ocupaci√≥n.

5) Imputaci√≥n del precio de listing con evidencia operativa y recalculo de buckets

listing_price_num imputado con: mediana de calendar por listing ‚Üí mediana por barrio ‚Üí mediana global. Se prioriza el precio observado (calendar) frente al de cat√°logo para contestar con mayor fidelidad ¬øcu√°l es la relaci√≥n entre el precio y la ocupaci√≥n a lo largo del tiempo? y evaluar variaciones en temporada baja. El paso por barrio captura gradientes espaciales del precio promedio.

Recalcular price_bucket tras imputar: crucial para que comparaciones por estrato de precio (ocupaci√≥n, calificaci√≥n, ingresos) no queden sesgadas por nulos o por cambios en la distribuci√≥n.

6) Construcci√≥n de la ‚Äús√°bana‚Äù diaria (flat sheet)

Por qu√© se escogieron esos campos para la s√°bana

Los campos seleccionados en base_keep representan los m√≠nimos necesarios para explicar la ocupaci√≥n y el desempe√±o de cada propiedad desde tres dimensiones complementarias:

Caracter√≠sticas del inmueble: tipo, tama√±o, n√∫mero de camas y ba√±os, amenities, etc.
‚Üí Permiten estudiar c√≥mo los atributos f√≠sicos y funcionales influyen en el precio, las calificaciones y la demanda.

Caracter√≠sticas del anfitri√≥n: tasas de respuesta y aceptaci√≥n, superhost, verificaciones, experiencia (host_since).
‚Üí Permiten analizar c√≥mo la gesti√≥n del anfitri√≥n impacta la ocupaci√≥n y la percepci√≥n del hu√©sped.

Rendimiento operativo: precio, disponibilidad, rese√±as, ocupaci√≥n e ingresos.
‚Üí Son las variables de salida que se explican a partir de las anteriores.

En conjunto, esta estructura integra la oferta (propiedad y host) con la demanda (ocupaci√≥n, rese√±as, ingresos), creando un dataset coherente para todas las preguntas de negocio.
El resultado es una s√°bana que combina contexto temporal, geogr√°fico y comportamental, apta para an√°lisis exploratorios, modelos predictivos y tableros de inteligencia.

Medidas diarias:

booked_night = ~available ‚Üí ocupaci√≥n a nivel de fecha-propiedad;

daily_revenue = booked_night * price_num ‚Üí ingresos diarios.
Estas dos m√©tricas permiten responder de forma directa: precio‚Äìocupaci√≥n en el tiempo, ocupaci√≥n anual, propiedades con mayor ocupaci√≥n, c√≥mo var√≠a el precio en temporada baja, y c√≥mo influyen rese√±as recientes en ocupaci√≥n e ingresos.

Join calendar ‚Üê atributos de listings (tipos de ID alineados): vincula la din√°mica diaria (ocupaci√≥n/precio) con el perfil de la propiedad y del host (superhost, verificaciones, tasas de respuesta/aceptaci√≥n, amenities), habilitando an√°lisis causales/explicativos.

reviews_in_month (conteo por listing/mes): proxy de actividad reciente de hu√©spedes; responde ¬øqu√© propiedades tienen m√°s rese√±as en L30D y c√≥mo influye en su ocupaci√≥n e ingresos?

Garant√≠as de no-nulos en campos cr√≠ticos (fechas de review, listing_price_num, price_bucket, reviews_in_month): aseguran que ninguna propiedad quede fuera de comparaciones clave (colonias, precio‚Äìocupaci√≥n, superhost, verificaciones, limpieza/comunicaci√≥n, disponibilidad en temporada baja).

Orden de columnas (fechas y m√©tricas diarias al frente): pensado para facilitar agregaciones y dashboards que responden r√°pidamente a todas las preguntas (por ejemplo, agrupar por colonia y quarter para detectar temporada alta y cruzar con calificaciones o ocupaci√≥n).

7) Logging integral

Se registran transformaciones aplicadas, tama√±os antes/despu√©s, y cuentas de imputaci√≥n por estrategia/fuente (host, grupo, barrio, global; reviews vs calendar para fechas; calendar/barrio/global para precio). Esto permite auditar si las conclusiones (p. ej., verificaci√≥n ‚Üî calificaci√≥n, superhost ‚Üî ocupaci√≥n/calificaci√≥n, localizaci√≥n ‚Üî precio, precio ‚Üî ocupaci√≥n) podr√≠an estar influidas por la limpieza y d√≥nde.
'''

import pandas as pd
import numpy as np
import unicodedata
import re
from logs import Logs

class Transformation:
    # ---------------------------------------------------------------------
    # Constructor
    # ---------------------------------------------------------------------
    def __init__(self, df_listings: pd.DataFrame, df_calendar: pd.DataFrame, df_reviews: pd.DataFrame):
        """
        Prop√≥sito:
          - Inicializar copias de dataframes y el manejador de logs.

        Logs:
          - Tama√±o y cantidad de columnas de cada dataframe.

        Retorna:
          - self (v√≠a m√©todos encadenables).
        """
        self.logs = Logs()
        self.listings = df_listings.copy()
        self.calendar = df_calendar.copy()
        self.reviews  = df_reviews.copy()
        self.flat_sheet = None

        self.logs.log(
            f"[INIT] Recibidos | "
            f"listings: {self.listings.shape} cols={len(self.listings.columns)} | "
            f"calendar: {self.calendar.shape} cols={len(self.calendar.columns)} | "
            f"reviews: {self.reviews.shape} cols={len(self.reviews.columns)}",
            "info"
        )

    # ------------------------ Helpers de normalizaci√≥n --------------------

    def _parse_any_date(self, v):
        """Convierte a datetime UTC valores string/dict/$date; NaT si falla."""
        if pd.isna(v): return pd.NaT
        if isinstance(v, dict) and '$date' in v: v = v['$date']
        return pd.to_datetime(v, errors='coerce', utc=True)

    def _to_iso_date(self, s: pd.Series) -> pd.Series:
        """Serie ‚Üí 'YYYY-MM-DD' (string ISO de fecha)."""
        return s.apply(self._parse_any_date).dt.strftime('%Y-%m-%d')

    def _to_iso_datetime(self, s: pd.Series) -> pd.Series:
        """Serie ‚Üí 'YYYY-MM-DDTHH:MM:SSZ' (string ISO datetime)."""
        return s.apply(self._parse_any_date).dt.strftime('%Y-%m-%dT%H:%M:%SZ')

    def _to_price_num(self, s: pd.Series) -> pd.Series:
        """Limpia '$' y comas ‚Üí float."""
        return (s.astype(str).str.replace(r'[\$,]', '', regex=True)
                             .replace({'': np.nan}).astype(float))

    def _to_percent_num(self, s: pd.Series) -> pd.Series:
        """Elimina '%' y 'N/A' ‚Üí float."""
        return (s.astype(str).str.replace('%', '', regex=False)
                             .replace({'': np.nan, 'N/A': np.nan}).astype(float))

    def _clean_text(self, s: pd.Series) -> pd.Series:
        """Quita HTML, normaliza Unicode, colapsa espacios, limpia 'nan'."""
        t = s.astype(str)
        t = t.str.replace(r'<br\s*/?>', ' ', flags=re.I, regex=True)
        t = t.str.replace(r'<[^>]+>', ' ', regex=True)
        t = t.apply(lambda x: unicodedata.normalize('NFC', x))
        t = t.str.replace(r'\s+', ' ', regex=True).str.strip()
        return t.replace({'None':'','nan':'','NaN':''})

    def _shape_str(self):
        """Devuelve resumen de formas de dataframes para logging."""
        return (f"listings={self.listings.shape} | "
                f"calendar={self.calendar.shape} | "
                f"reviews={self.reviews.shape}")

    # ---------------------------------------------------------------------
    # 1) Normalizar tipos
    # ---------------------------------------------------------------------
    def normalize_types(self):
        """
        Prop√≥sito:
          - Unificar tipos: fechas (‚Üí ISO string), precios (‚Üí float), % (‚Üí float),
            textos (‚Üí limpio) y dropear columnas no usadas.

        Transformaciones:
          - listings: fechas (last_scraped, host_since, first/last_review) a ISO.
          - calendar/reviews: date a ISO.
          - price ‚Üí price_num (float) y se elimina price.
          - host_*_rate ‚Üí *_pct (float) y se elimina original.
          - limpieza de texto en columnas descriptivas.
          - relleno de first_review/last_review con fuentes jer√°rquicas.

        Logs:
          - Inicio/fin, columnas eliminadas, columnas de % creadas.

        Retorna:
          - self
        """
        self.logs.log("[normalize_types] Inicio", "info")

        # Fechas -> ISO (strings)
        for c in ['last_scraped','calendar_last_scraped','host_since','first_review','last_review']:
            if c in self.listings.columns:
                self.listings[c] = self._to_iso_date(self.listings[c])
        if 'date' in self.calendar.columns:
            self.calendar['date'] = self._to_iso_date(self.calendar['date'])
        if 'date' in self.reviews.columns:
            self.reviews['date'] = self._to_iso_date(self.reviews['date'])

        # Calendar: columnas no usadas
        dropped = []
        for c in ['adjusted_price','_id']:
            if c in self.calendar.columns:
                self.calendar.drop(columns=[c], inplace=True, errors='ignore')
                dropped.append(c)
        if dropped:
            self.logs.log(f"[normalize_types] Calendar: drop {dropped}", "info")

        # Precios ‚Üí num√©rico y se elimina 'price'
        if 'price' in self.listings.columns and 'price_num' not in self.listings.columns:
            self.listings['price_num'] = self._to_price_num(self.listings['price'])
        if 'price' in self.listings.columns:
            self.listings.drop(columns=['price'], inplace=True, errors='ignore')

        # % ‚Üí num√©rico y se elimina original
        pct_new = []
        for c in ['host_response_rate','host_acceptance_rate']:
            if c in self.listings.columns:
                newc = c + '_pct'
                self.listings[newc] = self._to_percent_num(self.listings[c])
                self.listings.drop(columns=[c], inplace=True, errors='ignore')
                pct_new.append(newc)
        if pct_new:
            self.logs.log(f"[normalize_types] Listings: % ‚Üí {pct_new}", "info")

        # Calendar: precio ‚Üí num√©rico
        if 'price' in self.calendar.columns and 'price_num' not in self.calendar.columns:
            self.calendar['price_num'] = self._to_price_num(self.calendar['price'])
        if 'price' in self.calendar.columns:
            self.calendar.drop(columns=['price'], inplace=True, errors='ignore')

        # Limpieza de texto
        text_cols = ['name','description','neighborhood_overview','neighbourhood','neighbourhood_cleansed',
                     'property_type','room_type','host_name','host_location','host_neighbourhood','host_response_time']
        touched = [c for c in text_cols if c in self.listings.columns]
        for c in touched:
            self.listings[c] = self._clean_text(self.listings[c])
        if 'reviewer_name' in self.reviews.columns:
            self.reviews['reviewer_name'] = self._clean_text(self.reviews['reviewer_name'])
        if 'comments' in self.reviews.columns:
            self.reviews['comments'] = self._clean_text(self.reviews['comments'])
        if touched:
            self.logs.log(f"[normalize_types] Limpieza de texto: {len(touched)} cols", "info")

        # Completar fechas de review con jerarqu√≠a
        self._fill_review_dates()

        self.logs.log(f"[normalize_types] Fin | {self._shape_str()}", "info")
        return self

    # ---------------------------------------------------------------------
    # 2) Nulos e imputaciones
    # ---------------------------------------------------------------------
    def clean_nulls(self):
        """
        Prop√≥sito:
          - Imputar faltantes con estrategia jer√°rquica y red de seguridad.
          - Normalizar NA, booleanos y categ√≥ricos.
          - Tipificar num√©ricos y discretizar contadores.

        Transformaciones principales:
          - Drops por claves (id/host_id).
          - Flags *_was_missing para auditor√≠a.
          - Tasas host_*_pct: host ‚Üí grupo ‚Üí global ‚Üí clip 0..100.
          - Scores: grupo/barrio ‚Üí global ‚Üí winsor p1..p99.
          - reviews_per_month: NaN ‚Üí 0.0.
          - Lat/Lon: barrio ‚Üí global.
          - Contadores: grupo ‚Üí global ‚Üí enteros ‚â• 0.
          - Booleanos y categ√≥ricos: NaN ‚Üí False/mode/'unknown'.
          - host_since: last_scraped ‚Üí '1970-01-01T00:00:00Z'.
          - amenities/host_verifications: forzar listas.
          - calendar.available: normalizar 't'/'f' y NaN ‚Üí True.
          - minimum/maximum_nights: NaN ‚Üí mediana.
          - reviews: drop por cr√≠ticos; rellenar reviewer_name/comments.
          - Red de seguridad: completar cualquier NaN residual por tipo.

        Logs:
          - Filas eliminadas por claves.
          - Conteo por estrategia de imputaci√≥n en tasas, scores, coords, contadores.
          - Normalizaci√≥n de calendar.available y m√≠nimos/m√°ximos.
          - Limpieza universal aplicada por columna.

        Retorna:
          - self
        """
        self.logs.log("[clean_nulls] Inicio", "info")
        before_l = len(self.listings)
        before_c = len(self.calendar)
        before_r = len(self.reviews)

        # --- Protege claves obligatorias ---
        drop_counts = {}
        if 'id' in self.listings.columns:
            b = len(self.listings)
            self.listings = self.listings.dropna(subset=['id'])
            drop_counts['id'] = b - len(self.listings)
        if 'host_id' in self.listings.columns:
            b = len(self.listings)
            self.listings = self.listings.dropna(subset=['host_id'])
            drop_counts['host_id'] = drop_counts.get('host_id', 0) + (b - len(self.listings))
        if drop_counts:
            self.logs.log(f"[clean_nulls] Drops por claves: {drop_counts}", "warning")

        # --- Tipos num√©ricos esperados ---
        score_cols = [c for c in [
            'review_scores_rating','review_scores_cleanliness','review_scores_accuracy',
            'review_scores_communication','review_scores_checkin','review_scores_location','review_scores_value'
        ] if c in self.listings.columns]
        rate_cols = [c for c in ['host_response_rate_pct','host_acceptance_rate_pct'] if c in self.listings.columns]
        count_cols = [c for c in [
            'availability_30','availability_60','availability_90','availability_365',
            'number_of_reviews','number_of_reviews_ltm','number_of_reviews_l30d','number_of_reviews_ly',
            'accommodates','bedrooms','bathrooms','beds','host_total_listings_count'
        ] if c in self.listings.columns]

        # üö´ Evita que bathrooms pase por el cast‚Üífloor‚Üíint
        if 'bathrooms' in count_cols:
            count_cols.remove('bathrooms')

        for c in score_cols + rate_cols + count_cols + ['price_num','reviews_per_month','latitude','longitude']:
            if c in self.listings.columns:
                self.listings[c] = pd.to_numeric(self.listings[c], errors='coerce')

        # --- NA estandarizados y textos base ---
        na_like = {'N/A': np.nan, 'n/a': np.nan, 'NA': np.nan, 'na': np.nan, 'None': np.nan, '': np.nan}
        self.listings.replace(na_like, inplace=True)
        if 'host_name' in self.listings.columns:
            self.listings['host_name'] = self.listings['host_name'].fillna('unknown host')
        if 'host_response_time' in self.listings.columns:
            self.listings['host_response_time'] = self.listings['host_response_time'].fillna('unknown')

        # --- Banderas de faltantes antes de imputar ---
        for c in score_cols + rate_cols + ['latitude','longitude','reviews_per_month']:
            if c in self.listings.columns:
                self.listings[f'{c}_was_missing'] = self.listings[c].isna().astype(int)

        # --- Claves de agrupaci√≥n para imputar ---
        group_keys = [k for k in ['neighbourhood_cleansed','room_type','accommodates'] if k in self.listings.columns]

        # --- Tasas del host: host ‚Üí grupo ‚Üí global ‚Üí clip ---
        if 'host_id' in self.listings.columns:
            for c in rate_cols:
                filled_host = filled_grp = 0
                m_init = self.listings[c].isna().sum()
                by_host = self.listings.groupby('host_id')[c].median(numeric_only=True)
                m = self.listings[c].isna()
                if not by_host.empty:
                    before_na = m.sum()
                    self.listings.loc[m, c] = self.listings.loc[m, 'host_id'].map(by_host)
                    filled_host = before_na - self.listings[c].isna().sum()
                if group_keys:
                    by_grp = self.listings.groupby(group_keys)[c].median(numeric_only=True)
                    m = self.listings[c].isna()
                    if not by_grp.empty and m.any():
                        before_na = m.sum()
                        self.listings.loc[m, c] = self.listings.loc[m, group_keys].apply(
                            lambda r: by_grp.get(tuple(r.values), np.nan), axis=1
                        )
                        filled_grp = before_na - self.listings[c].isna().sum()
                m = self.listings[c].isna()
                glob = self.listings[c].median()
                filled_glob = m.sum()
                self.listings[c] = self.listings[c].fillna(glob).clip(0, 100)
                self.logs.log(
                    f"[clean_nulls] {c}: host={filled_host} grupo={filled_grp} global={filled_glob}",
                    "info"
                )

        # --- Scores: grupo/barrio ‚Üí global ‚Üí winsor p1..p99 ---
        for c in score_cols:
            filled_grp = filled_nb = 0
            if group_keys:
                by_grp = self.listings.groupby(group_keys)[c].median(numeric_only=True)
                m = self.listings[c].isna()
                if not by_grp.empty and m.any():
                    b = m.sum()
                    self.listings.loc[m, c] = self.listings.loc[m, group_keys].apply(
                        lambda r: by_grp.get(tuple(r.values), np.nan), axis=1
                    )
                    filled_grp = b - self.listings[c].isna().sum()
            if 'neighbourhood_cleansed' in self.listings.columns:
                by_nb = self.listings.groupby('neighbourhood_cleansed')[c].median(numeric_only=True)
                m = self.listings[c].isna()
                if not by_nb.empty and m.any():
                    b = m.sum()
                    self.listings.loc[m, c] = self.listings.loc[m, 'neighbourhood_cleansed'].map(by_nb)
                    filled_nb = b - self.listings[c].isna().sum()
            m = self.listings[c].isna()
            glob = self.listings[c].median()
            filled_glob = m.sum()
            self.listings[c] = self.listings[c].fillna(glob)
            v = pd.to_numeric(self.listings[c], errors='coerce')
            lo, hi = v.quantile(0.01), v.quantile(0.99)
            self.listings[c] = v.clip(lower=lo if np.isfinite(lo) else v.min(),
                                      upper=hi if np.isfinite(hi) else v.max())
            self.logs.log(
                f"[clean_nulls] {c}: grupo={filled_grp} barrio={filled_nb} global={filled_glob} winsor p1..p99",
                "info"
            )

        # --- Reviews/mes: NaN ‚Üí 0.0 ---
        if 'reviews_per_month' in self.listings.columns:
            n_na = self.listings['reviews_per_month'].isna().sum()
            self.listings['reviews_per_month'] = self.listings['reviews_per_month'].fillna(0.0)
            if n_na:
                self.logs.log(f"[clean_nulls] reviews_per_month: {n_na} ‚Üí 0.0", "info")


        # --- Coordenadas: barrio ‚Üí global ---
        for c in ['latitude','longitude']:
            if c in self.listings.columns:
                filled_nb = 0
                if 'neighbourhood_cleansed' in self.listings.columns:
                    by_nb = self.listings.groupby('neighbourhood_cleansed')[c].median(numeric_only=True)
                    m = self.listings[c].isna()
                    if not by_nb.empty and m.any():
                        b = m.sum()
                        self.listings.loc[m, c] = self.listings.loc[m, 'neighbourhood_cleansed'].map(by_nb)
                        filled_nb = b - self.listings[c].isna().sum()
                m = self.listings[c].isna()
                glob = self.listings[c].median()
                filled_glob = m.sum()
                self.listings[c] = self.listings[c].fillna(glob)
                self.logs.log(f"[clean_nulls] {c}: barrio={filled_nb} global={filled_glob}", "info")

        # --- bathrooms: imputar SOLO nulos (preservando decimales) ---
        if 'bathrooms' in self.listings.columns:
            # Asegurar tipo num√©rico (sin alterar los no nulos)
            self.listings['bathrooms'] = pd.to_numeric(self.listings['bathrooms'], errors='coerce')

            imputados = 0
            if 'bathrooms_text' in self.listings.columns:
                mask = self.listings['bathrooms'].isna() & self.listings['bathrooms_text'].notna()
                if mask.any():
                    extra = (self.listings.loc[mask, 'bathrooms_text']
                            .astype(str)
                            .str.extract(r'(\d+(\.\d+)?)')[0]
                            .astype(float))
                    self.listings.loc[mask, 'bathrooms'] = extra
                    imputados = int(mask.sum())

            # Si quedaron NaN sin texto de apoyo, decide tu red de seguridad:
            restantes = int(self.listings['bathrooms'].isna().sum())
            if imputados:
                self.logs.log(
                    f"[clean_nulls] bathrooms: imputados {imputados} desde bathrooms_text (sin redondear).",
                    "info"
                )
            if restantes:
                # Pol√≠tica m√≠nima: rellena con 0.0 y registra WARNING
                self.listings['bathrooms'] = self.listings['bathrooms'].fillna(0.0)
                self.logs.log(
                    f"[clean_nulls] bathrooms: {restantes} sin fuente ‚Üí 0.0.",
                    "warning"
                )

        # --- Contadores: grupo ‚Üí global ‚Üí enteros ‚â• 0 ---
        for c in count_cols:
            filled_grp = 0
            if group_keys:
                by_grp = self.listings.groupby(group_keys)[c].median(numeric_only=True)
                m = self.listings[c].isna()
                if not by_grp.empty and m.any():
                    b = m.sum()
                    self.listings.loc[m, c] = self.listings.loc[m, group_keys].apply(
                        lambda r: by_grp.get(tuple(r.values), np.nan), axis=1
                    )
                    filled_grp = b - self.listings[c].isna().sum()
            m = self.listings[c].isna()
            glob = self.listings[c].median()
            filled_glob = m.sum()
            self.listings[c] = self.listings[c].fillna(glob)
            self.listings[c] = np.maximum(0, np.floor(pd.to_numeric(self.listings[c], errors='coerce'))).astype(int)
            self.logs.log(f"[clean_nulls] {c}: grupo={filled_grp} global={filled_glob} cast‚Üíint‚â•0", "info")

        # --- Booleanos y categ√≥ricos ---
        for c in ['host_is_superhost','host_has_profile_pic','host_identity_verified','instant_bookable']:
            if c in self.listings.columns:
                n_na = self.listings[c].isna().sum()
                self.listings[c] = self.listings[c].fillna(False)
                if n_na:
                    self.logs.log(f"[clean_nulls] {c}: {n_na} ‚Üí False", "info")
        for c in ['property_type','room_type','neighbourhood_cleansed']:
            if c in self.listings.columns:
                mode = self.listings[c].mode(dropna=True)
                n_na = self.listings[c].isna().sum()
                self.listings[c] = self.listings[c].fillna(mode.iloc[0] if not mode.empty else 'unknown')
                if n_na:
                    self.logs.log(f"[clean_nulls] {c}: {n_na} ‚Üí mode/unknown", "info")

        # --- host_since: fallback en cascada ---
        if 'host_since' in self.listings.columns:
            mask = self.listings['host_since'].isna() | (self.listings['host_since'].astype(str).str.strip() == '')
            n_mask = mask.sum()
            if 'last_scraped' in self.listings.columns and n_mask:
                self.listings.loc[mask, 'host_since'] = self.listings.loc[mask, 'last_scraped']
                mask = self.listings['host_since'].isna() | (self.listings['host_since'].astype(str).str.strip() == '')
            n_final = mask.sum()
            if n_final:
                self.listings.loc[mask, 'host_since'] = '1970-01-01T00:00:00Z'
            self.logs.log(f"[clean_nulls] host_since: desde last_scraped={n_mask - n_final} | a 1970={n_final}", "info")

        # --- Forzar listas en campos anidados ---
        for c in ['amenities','host_verifications']:
            if c in self.listings.columns:
                def as_list(x):
                    if isinstance(x, list): return x
                    import ast
                    if isinstance(x, str) and x.strip().startswith('['):
                        try: return ast.literal_eval(x)
                        except: return []
                    if pd.isna(x) or x == '': return []
                    return [str(x)]
                self.listings[c] = self.listings[c].apply(as_list)

        # --- Calendar: available y noches m√≠n/max ---
        if 'available' in self.calendar.columns:
            n_na = self.calendar['available'].isna().sum()
            self.calendar['available'] = self.calendar['available'].map({'t': True, 'f': False, True: True, False: False})
            self.calendar['available'] = self.calendar['available'].fillna(True)
            if n_na:
                self.logs.log(f"[clean_nulls] calendar.available: {n_na} ‚Üí True (normalizado)", "info")
        for c in ['minimum_nights','maximum_nights']:
            if c in self.calendar.columns:
                med = pd.to_numeric(self.calendar[c], errors='coerce').median()
                n_na = self.calendar[c].isna().sum()
                self.calendar[c] = pd.to_numeric(self.calendar[c], errors='coerce').fillna(med)
                if n_na:
                    self.logs.log(f"[clean_nulls] calendar.{c}: {n_na} ‚Üí mediana={med}", "info")
        if 'price_num' in self.calendar.columns:
            self.calendar['price_num'] = pd.to_numeric(self.calendar['price_num'], errors='coerce')

        # --- Reviews: drops por cr√≠ticos y rellenos base ---
        before_rev = len(self.reviews)
        crit = [c for c in ['listing_id','id','date'] if c in self.reviews.columns]
        if crit:
            self.reviews = self.reviews.dropna(subset=crit)
            dropped = before_rev - len(self.reviews)
            if dropped:
                self.logs.log(f"[clean_nulls] Reviews: drops por {crit} = {dropped}", "warning")
        if 'reviewer_name' in self.reviews.columns:
            n_na = self.reviews['reviewer_name'].isna().sum()
            self.reviews['reviewer_name'] = self.reviews['reviewer_name'].fillna('unknown')
            if n_na:
                self.logs.log(f"[clean_nulls] reviewer_name: {n_na} ‚Üí 'unknown'", "info")
        if 'comments' in self.reviews.columns:
            n_na = self.reviews['comments'].isna().sum()
            self.reviews['comments'] = self.reviews['comments'].fillna('')
            if n_na:
                self.logs.log(f"[clean_nulls] comments: {n_na} ‚Üí ''", "info")

        # --- Red de seguridad: completa cualquier NaN residual por tipo ---
        def fill_all_nulls(df, name):
            changed = {}
            for c in df.columns:
                if df[c].isna().any():
                    n_na = df[c].isna().sum()
                    if pd.api.types.is_numeric_dtype(df[c]):
                        med = pd.to_numeric(df[c], errors='coerce').median()
                        df[c] = pd.to_numeric(df[c], errors='coerce').fillna(med)
                        changed[c] = f"num‚Üímediana({med}) {n_na}"
                    elif pd.api.types.is_bool_dtype(df[c]):
                        df[c] = df[c].fillna(False)
                        changed[c] = f"bool‚ÜíFalse {n_na}"
                    elif pd.api.types.is_datetime64_any_dtype(df[c]):
                        df[c] = df[c].fillna(pd.Timestamp('1970-01-01T00:00:00Z'))
                        changed[c] = f"datetime‚Üí1970 {n_na}"
                    elif df[c].apply(lambda x: isinstance(x, list)).any():
                        df[c] = df[c].apply(lambda x: [] if pd.isna(x) else x)
                        changed[c] = f"list‚Üí[] {n_na}"
                    else:
                        df[c] = df[c].astype(object).fillna('unknown')
                        changed[c] = f"str‚Üí'unknown' {n_na}"
            if changed:
                self.logs.log(f"[clean_nulls] Red de seguridad {name}: {changed}", "info")
            return df

        self.listings = fill_all_nulls(self.listings, "listings")
        self.calendar = fill_all_nulls(self.calendar, "calendar")
        self.reviews  = fill_all_nulls(self.reviews,  "reviews")

        # --- Resumen final de limpieza ---
        self.logs.log(
            f"[clean_nulls] Fin | tama√±os antes L:{before_l} C:{before_c} R:{before_r} | "
            f"despu√©s L:{len(self.listings)} C:{len(self.calendar)} R:{len(self.reviews)}",
            "info"
        )
        return self

    # ---------------------------------------------------------------------
    # 3) Rasgos de fecha y buckets de precio
    # ---------------------------------------------------------------------
    def derive_features(self, price_mode='quantile', price_bins=None, price_labels=None):
        """
        Prop√≥sito:
          - Crear partes de fecha en calendar y buckets de precio en listings/calendar.

        Transformaciones:
          - calendar.date ‚Üí year, month, day, quarter, week.
          - price_num ‚Üí price_bucket (listings) / daily_price_bucket (calendar)
            usando quintiles por defecto o bins fijos.

        Logs:
          - Inicio/fin y avisos si qcut/cut falla.

        Retorna:
          - self
        """
        self.logs.log(f"[derive_features] Inicio | price_mode={price_mode}", "info")

        # Partes de fecha
        if 'date' in self.calendar.columns:
            dt = pd.to_datetime(self.calendar['date'], errors='coerce', utc=True)
            self.calendar['year']       = dt.dt.year
            self.calendar['month']      = dt.dt.month
            self.calendar['day']        = dt.dt.day
            self.calendar['quarter']    = dt.dt.quarter
            # Semana del mes (1 a 5 normalmente)
            self.calendar['week_of_month'] = (
                ((dt.dt.day - 1) // 7 + 1)
            )

        # Buckets de precio
        default_labels = ['Very Low','Low','Medium','High','Very High']
        if 'price_num' in self.listings.columns:
            s = pd.to_numeric(self.listings['price_num'], errors='coerce')
            try:
                if price_mode == 'quantile':
                    self.listings['price_bucket'] = pd.qcut(s, q=5, labels=price_labels or default_labels, duplicates='drop')
                elif price_mode == 'fixed' and price_bins is not None:
                    self.listings['price_bucket'] = pd.cut(s, bins=price_bins, labels=price_labels, include_lowest=True)
            except Exception as e:
                self.logs.log(f"[derive_features] price_bucket listings: fallo qcut/cut: {e}", "warning")

        if 'price_num' in self.calendar.columns:
            s = pd.to_numeric(self.calendar['price_num'], errors='coerce')
            try:
                if price_mode == 'quantile':
                    self.calendar['daily_price_bucket'] = pd.qcut(s, q=5, labels=price_labels or default_labels, duplicates='drop')
                elif price_mode == 'fixed' and price_bins is not None:
                    self.calendar['daily_price_bucket'] = pd.cut(s, bins=price_bins, labels=price_labels, include_lowest=True)
            except Exception as e:
                self.logs.log(f"[derive_features] daily_price_bucket calendar: fallo qcut/cut: {e}", "warning")

        self.logs.log(f"[derive_features] Fin | {self._shape_str()}", "info")
        return self

    # ---------------------------------------------------------------------
    # 4) Expandir campos anidados (amenities/verifications)
    # ---------------------------------------------------------------------
    def expand_nested_fields(self):
        """
        Prop√≥sito:
          - Convertir 'amenities' y 'host_verifications' a columnas dummies,
            usando un conjunto curado (evita explosi√≥n de columnas).

        Transformaciones:
          - Forzar listas v√°lidas.
          - Normalizar a 'slugs' y crear dummies 'amenities_*' y 'host_verifications_*'.
          - Eliminar columnas originales anidadas.

        Logs:
          - Cantidad de columnas nuevas creadas por cada grupo.

        Retorna:
          - self
        """
        self.logs.log("[expand_nested_fields] Inicio", "info")

        amenities_keep = [
            "Wifi", "Dedicated workspace",
            "Self check-in", "Free parking on premises",
            "Kitchen", "Refrigerator", "Microwave", "Stove", "Coffee maker",
            "Cooking basics", "Dishes and silverware",
            "Room-darkening shades", "Essentials",
            "Washer",
            "Smoke alarm", "Carbon monoxide alarm", "Fire extinguisher", "First aid kit"
        ]
        verifications_keep = ["phone", "email", "work_email"]

        import ast
        df = self.listings

        def safe_slug(x: str) -> str:
            x = unicodedata.normalize('NFKD', str(x)).encode('ascii', 'ignore').decode('ascii')
            return re.sub(r'[^0-9a-zA-Z]+', '_', x).strip('_').lower()

        am_keep = set(safe_slug(a) for a in (amenities_keep or []))
        vr_keep = set(safe_slug(v) for v in (verifications_keep or []))

        # Asegurar listas
        for col in ['amenities', 'host_verifications']:
            if col in df.columns:
                def to_list(x):
                    if isinstance(x, list): return x
                    if isinstance(x, str) and x.strip().startswith('['):
                        try: return ast.literal_eval(x)
                        except: return []
                    if pd.isna(x) or x == '': return []
                    return [str(x)]
                df[col] = df[col].apply(to_list)

        # Dummies de amenities (subset)
        new_am_cols = []
        if 'amenities' in df.columns and am_keep:
            am_col_slugs = df['amenities'].apply(lambda lst: set(safe_slug(x) for x in lst if isinstance(x, str)))
            for a_slug in am_keep:
                col_name = f"amenities_{a_slug}"
                df[col_name] = am_col_slugs.apply(lambda s: int(a_slug in s))
                new_am_cols.append(col_name)
            df.drop(columns=['amenities'], inplace=True, errors='ignore')

        # Dummies de verifications (subset)
        new_vr_cols = []
        if 'host_verifications' in df.columns and vr_keep:
            vr_col_slugs = df['host_verifications'].apply(lambda lst: set(safe_slug(x) for x in lst if isinstance(x, str)))
            for v_slug in vr_keep:
                col_name = f"host_verifications_{v_slug}"
                df[col_name] = vr_col_slugs.apply(lambda s: int(v_slug in s))
                new_vr_cols.append(col_name)
            df.drop(columns=['host_verifications'], inplace=True, errors='ignore')

        self.listings = df
        self.logs.log(
            f"[expand_nested_fields] Fin | amenities={len(new_am_cols)} verifications={len(new_vr_cols)}",
            "info"
        )
        return self

    # ----------------- Fechas de review e imputaci√≥n de precio -----------------

    def _fill_review_dates(self):
        """
        Prop√≥sito:
          - Completar first_review/last_review de listings con:
            1) min/max en reviews, 2) rango en calendar, 3) last_scraped,
            4) fallback '1970-01-01'.

        Logs:
          - Conteo por fuente (reviews/calendar/last_scraped/fallback) para first/last.

        Retorna:
          - self
        """
        self.logs.log("[_fill_review_dates] Inicio", "info")

        lst = self.listings
        cal = self.calendar
        rev = self.reviews

        # Fechas desde reviews
        if 'listing_id' in rev.columns and 'date' in rev.columns:
            rdt = pd.to_datetime(rev['date'], errors='coerce', utc=True)
            grp = (rev.assign(_dt=rdt).groupby('listing_id')['_dt'].agg(['min','max']).reset_index())
            grp['first_review_from_rev'] = grp['min'].dt.strftime('%Y-%m-%d')
            grp['last_review_from_rev']  = grp['max'].dt.strftime('%Y-%m-%d')
            m1_first = grp.set_index('listing_id')['first_review_from_rev']
            m1_last  = grp.set_index('listing_id')['last_review_from_rev']
        else:
            m1_first = pd.Series(dtype=object)
            m1_last  = pd.Series(dtype=object)

        # Rango desde calendar
        if 'listing_id' in cal.columns and 'date' in cal.columns:
            cdt = pd.to_datetime(cal['date'], errors='coerce', utc=True)
            cgrp = (cal.assign(_dt=cdt).groupby('listing_id')['_dt'].agg(['min','max']).reset_index())
            cgrp['first_review_from_cal'] = cgrp['min'].dt.strftime('%Y-%m-%d')
            cgrp['last_review_from_cal']  = cgrp['max'].dt.strftime('%Y-%m-%d')
            m2_first = cgrp.set_index('listing_id')['first_review_from_cal']
            m2_last  = cgrp.set_index('listing_id')['last_review_from_cal']
        else:
            m2_first = pd.Series(dtype=object)
            m2_last  = pd.Series(dtype=object)

        # Asegurar columnas
        if 'first_review' not in lst.columns: lst['first_review'] = ''
        if 'last_review'  not in lst.columns: lst['last_review']  = ''

        # Relleno con conteo de fuentes
        src_counts = {"reviews_first":0, "calendar_first":0, "last_scraped_first":0, "fallback_first":0,
                      "reviews_last":0,  "calendar_last":0,  "last_scraped_last":0,  "fallback_last":0}

        def _fill_by_map(colname, mapper1, mapper2, src_prefix):
            id_map1 = lst['id'].map(mapper1) if not mapper1.empty else None
            id_map2 = lst['id'].map(mapper2) if not mapper2.empty else None
            mask = (lst[colname].isna()) | (lst[colname].astype(str).str.strip() == '')
            # 1) reviews
            if id_map1 is not None and mask.any():
                to_fill = mask & id_map1.notna()
                src_counts[f"reviews_{src_prefix}"] += int(to_fill.sum())
                lst.loc[to_fill, colname] = lst.loc[to_fill, colname].fillna(id_map1[to_fill])
            # 2) calendar
            mask = (lst[colname].isna()) | (lst[colname].astype(str).str.strip() == '')
            if id_map2 is not None and mask.any():
                to_fill = mask & id_map2.notna()
                src_counts[f"calendar_{src_prefix}"] += int(to_fill.sum())
                lst.loc[to_fill, colname] = lst.loc[to_fill, colname].fillna(id_map2[to_fill])
            # 3) last_scraped
            mask = (lst[colname].isna()) | (lst[colname].astype(str).str.strip() == '')
            if 'last_scraped' in lst.columns and mask.any():
                src_counts[f"last_scraped_{src_prefix}"] += int(mask.sum())
                lst.loc[mask, colname] = lst.loc[mask, 'last_scraped']
            # 4) fallback fijo
            mask = (lst[colname].isna()) | (lst[colname].astype(str).str.strip() == '')
            src_counts[f"fallback_{src_prefix}"] += int(mask.sum())
            lst.loc[mask, colname] = '1970-01-01'

        _fill_by_map('first_review', m1_first, m2_first, 'first')
        _fill_by_map('last_review',  m1_last,  m2_last,  'last')

        self.listings = lst
        self.logs.log(f"[_fill_review_dates] Fin | fuentes: {src_counts}", "info")
        return self

    def _impute_listing_prices_and_buckets(self):
        """
        Prop√≥sito:
          - Imputar 'listing_price_num' con:
            1) mediana en calendar por listing,
            2) mediana por barrio,
            3) mediana global.
          - Garantizar y recalcular 'price_bucket'.

        Logs:
          - Cantidad imputada por fuente y modo de bucket usado.

        Retorna:
          - self
        """
        self.logs.log("[_impute_listing_prices_and_buckets] Inicio", "info")

        lst = self.listings
        cal = self.calendar

        # Asegurar columna base
        if 'price_num' in lst.columns and 'listing_price_num' not in lst.columns:
            lst['listing_price_num'] = pd.to_numeric(lst['price_num'], errors='coerce')
        if 'listing_price_num' not in lst.columns:
            lst['listing_price_num'] = np.nan

        filled_cal = filled_nb = 0

        # 1) Mediana calendar por listing
        if {'listing_id','price_num'}.issubset(cal.columns):
            med_cal = (cal.groupby('listing_id')['price_num'].median(numeric_only=True))
            mask = lst['listing_price_num'].isna()
            before = mask.sum()
            lst['listing_price_num'] = lst['listing_price_num'].fillna(lst['id'].map(med_cal))
            filled_cal = before - lst['listing_price_num'].isna().sum()

        # 2) Mediana por barrio
        if 'neighbourhood_cleansed' in lst.columns:
            mask = lst['listing_price_num'].isna()
            nb_med = (lst.groupby('neighbourhood_cleansed')['listing_price_num'].median(numeric_only=True))
            before = mask.sum()
            lst.loc[mask, 'listing_price_num'] = lst.loc[mask, 'neighbourhood_cleansed'].map(nb_med)
            filled_nb = before - lst['listing_price_num'].isna().sum()

        # 3) Mediana global
        mask = lst['listing_price_num'].isna()
        glob_med = pd.to_numeric(lst['listing_price_num'], errors='coerce').median()
        filled_glob = mask.sum()
        lst['listing_price_num'] = lst['listing_price_num'].fillna(glob_med)

        # Buckets (qcut o fallback por cuantiles)
        s = pd.to_numeric(lst['listing_price_num'], errors='coerce')
        default_labels = ['Very Low','Low','Medium','High','Very High']
        bucket_mode = "qcut"
        try:
            lst['price_bucket'] = pd.qcut(s, q=5, labels=default_labels, duplicates='drop')
        except Exception:
            bucket_mode = "cut-quantiles"
            qs = s.quantile([0, .2, .4, .6, .8, 1.0]).values
            bins = np.unique(qs)
            if len(bins) < 3:
                lst['price_bucket'] = 'Medium'
            else:
                labels = default_labels[:len(bins)-1]
                lst['price_bucket'] = pd.cut(s, bins=bins, labels=labels, include_lowest=True)

        # NaN residuales ‚Üí 'Medium'
        if lst['price_bucket'].isna().any():
            n_na = lst['price_bucket'].isna().sum()
            lst['price_bucket'] = lst['price_bucket'].astype(object).fillna('Medium')
            self.logs.log(f"[_impute_listing_prices_and_buckets] price_bucket NaN‚Üí'Medium' ({n_na})", "warning")

        self.listings = lst
        self.logs.log(
            f"[_impute_listing_prices_and_buckets] Fin | from_calendar={filled_cal} "
            f"| by_neighbourhood={filled_nb} | global={filled_glob} | bucket_mode={bucket_mode}",
            "info"
        )
        return self

    # ---------------------------------------------------------------------
    # 5) Construcci√≥n de la s√°bana
    # ---------------------------------------------------------------------
    def build_flat_sheet(self):
        """
        Prop√≥sito:
          - Unir calendar con atributos de listing imputados y agregar reviews/mes.
          - Derivar booked_night y daily_revenue.

        Transformaciones:
          - booked_night = ~available; daily_revenue = booked_night * price_num.
          - Dimensi√≥n de listing: renombrar id ‚Üí listing_id y price_num ‚Üí listing_price_num.
          - Join calendar ‚Üê listings_dim por listing_id (alineando tipos).
          - reviews_in_month: conteo por (listing_id, year, month).
          - Garant√≠as de no-nulo: reviews_in_month‚Üí0; first/last_review‚Üí'1970-01-01';
            listing_price_num‚Üímediana; price_bucket‚Üí'Medium'.
          - Reordenar columnas clave al frente.

        Logs:
          - Inicio/fin, tama√±o de resultado, columnas a√±adidas, normalizaciones.

        Retorna:
          - self
        """
        self.logs.log("[build_flat_sheet] Inicio", "info")

        cal = self.calendar.copy()
        lst = self.listings.copy()
        rev = self.reviews.copy()

        # Medidas diarias
        if 'available' in cal.columns:
            cal['booked_night'] = (~cal['available']).astype(int)
        else:
            cal['booked_night'] = np.nan
        if 'price_num' in cal.columns:
            cal['daily_revenue'] = cal['booked_night'] * cal['price_num']

        # Dummies ya expandidos
        amen_cols  = [c for c in lst.columns if c.startswith('amenities_')]
        verif_cols = [c for c in lst.columns if c.startswith('host_verifications_')]

        # Whitelist base
        base_keep = [
            'id','name','host_id','host_name','host_since','neighbourhood_cleansed','property_type','room_type',
            'accommodates','bedrooms','bathrooms','beds','instant_bookable',
            'latitude','longitude',
            'price_num','price_bucket',
            'review_scores_rating','review_scores_cleanliness','review_scores_accuracy','review_scores_communication',
            'review_scores_checkin','review_scores_location','review_scores_value','reviews_per_month',
            'host_is_superhost','host_identity_verified','host_has_profile_pic',
            'host_response_rate_pct','host_acceptance_rate_pct','host_response_time','host_total_listings_count',
            'availability_30','availability_60','availability_90','availability_365','number_of_reviews','number_of_reviews_ltm',
            'number_of_reviews_l30d','number_of_reviews_ly','first_review','last_review'
        ]
        keep = [c for c in (base_keep + amen_cols + verif_cols) if c in lst.columns]

        # Imputar precios/buckets antes del join
        self._impute_listing_prices_and_buckets()

        # Dimensi√≥n de listing para join
        lst = self.listings
        keep = [c for c in (base_keep + amen_cols + verif_cols) if c in lst.columns]
        lst_dim = lst[keep].rename(columns={'id':'listing_id', 'price_num':'listing_price_num'})

        # Join calendar ‚Üê listings_dim
        if 'listing_id' not in cal.columns:
            self.flat_sheet = cal.copy()
            self.logs.log("[build_flat_sheet] calendar SIN listing_id ‚Üí se devuelve calendar sin join", "warning")
            return self
        lst_dim['listing_id'] = lst_dim['listing_id'].astype(str)
        cal['listing_id'] = cal['listing_id'].astype(str)
        before_join_rows = len(cal)
        flat = cal.merge(lst_dim, on='listing_id', how='left')
        self.logs.log(
            f"[build_flat_sheet] Join: filas calendar={before_join_rows} | "
            f"resultado={len(flat)} | col_listings_dim={len(lst_dim.columns)-1}",
            "info"
        )

        # Reviews por mes
        if 'date' in rev.columns:
            rdt = pd.to_datetime(rev['date'], errors='coerce', utc=True)
            rev['rev_year'], rev['rev_month'] = rdt.dt.year, rdt.dt.month
        if {'listing_id','rev_year','rev_month'}.issubset(rev.columns) and {'year','month'}.issubset(flat.columns):
            rev['listing_id'] = rev['listing_id'].astype(str)
            rmon = (rev.groupby(['listing_id','rev_year','rev_month']).size().reset_index(name='reviews_in_month'))
            flat = flat.merge(rmon, left_on=['listing_id','year','month'],
                              right_on=['listing_id','rev_year','rev_month'], how='left')
            flat.drop(columns=[c for c in ['rev_year','rev_month'] if c in flat.columns], inplace=True)
        else:
            flat['reviews_in_month'] = np.nan

        # Garant√≠as de no-nulo
        if 'reviews_in_month' in flat.columns:
            n_na = flat['reviews_in_month'].isna().sum()
            flat['reviews_in_month'] = flat['reviews_in_month'].fillna(0).astype(int)
            if n_na:
                self.logs.log(f"[build_flat_sheet] reviews_in_month: {n_na} ‚Üí 0", "info")

        for c in ['first_review','last_review']:
            if c in flat.columns:
                mask = (flat[c].isna()) | (flat[c].astype(str).str.strip() == '')
                n_fill = mask.sum()
                flat.loc[mask, c] = '1970-01-01'
                if n_fill:
                    self.logs.log(f"[build_flat_sheet] {c}: vac√≠os‚Üí'1970-01-01' ({n_fill})", "info")

        if 'listing_price_num' in flat.columns:
            med_lp = pd.to_numeric(flat['listing_price_num'], errors='coerce').median()
            n_na = flat['listing_price_num'].isna().sum()
            flat['listing_price_num'] = pd.to_numeric(flat['listing_price_num'], errors='coerce').fillna(med_lp)
            if n_na:
                self.logs.log(f"[build_flat_sheet] listing_price_num: {n_na} ‚Üí mediana={med_lp}", "info")

        if 'price_bucket' in flat.columns:
            n_na = flat['price_bucket'].isna().sum()
            flat['price_bucket'] = flat['price_bucket'].astype(object).fillna('Medium')
            if n_na:
                self.logs.log(f"[build_flat_sheet] price_bucket: {n_na} ‚Üí 'Medium'", "info")

        # Ordenar columnas clave primero
        front = ['listing_id','date','year','month','day','quarter','weekday','is_weekend',
                 'price_num','daily_price_bucket','available','booked_night','daily_revenue']
        front = [c for c in front if c in flat.columns]
        others = [c for c in flat.columns if c not in front]
        self.flat_sheet = flat[front + others]

        self.logs.log(f"[build_flat_sheet] Fin | flat_sheet={self.flat_sheet.shape}", "info")
        return self

    # ---------------------------------------------------------------------
    # 6) Pipeline completo
    # ---------------------------------------------------------------------
    def run(self, price_mode='quantile', price_bins=None, price_labels=None):
        """
        Prop√≥sito:
          - Ejecutar el flujo completo y devolver la s√°bana final.

        Logs:
          - Par√°metros de ejecuci√≥n y forma final de la s√°bana.

        Retorna:
          - pd.DataFrame (flat_sheet)
        """
        self.logs.log(
            f"[run] Inicio | price_mode={price_mode}, "
            f"price_bins={'set' if price_bins is not None else None}, "
            f"price_labels={'set' if price_labels is not None else None}",
            "info"
        )

        (self.normalize_types()
             .clean_nulls()
             .derive_features(price_mode=price_mode, price_bins=price_bins, price_labels=price_labels)
             .expand_nested_fields()
             .build_flat_sheet())

        self.logs.log(f"[run] Fin | flat_sheet={self.flat_sheet.shape}", "info")
        return self.flat_sheet
